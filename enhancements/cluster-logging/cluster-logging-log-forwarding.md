---
title: cluster-logging-log-forwarding
authors:
  - "@jcantrill"
  - "@jaosorior"
  - "@alanconway"
reviewers:
  - "@bparees"
  - "@ewolinetz"
  - "@jeremyeder"
approvers:
  - "@bparees"
  - "@ewolinetz"
creation-date: 2019-09-17
last-updated: 2020-02-28
status: implementable
see-also:[]
replaces:[]
superseded-by:[]
---

# cluster-logging-log-forwarding

## Release Signoff Checklist

- [X] Enhancement is `implementable`
- [X] Design details are appropriately documented from clear requirements
- [X] Test plan is defined
- [X] Graduation criteria for dev preview, tech preview, GA
- [ ] User-facing documentation is created in [openshift-docs](https://github.com/openshift/openshift-docs/)

## Summary

The purpose of log forwarding is to provide a declarative way by which adopters of cluster logging can ship
container and node logs to destinations that are not necessarily managed by the OKD cluster logging infrastructure.
Destinations are either on or off cluster endpoints such as the cluster logging provided Elasticsearch, an organization's Kafka message bus, a syslog server, etc.

This document describes the initial release goals for log forwarding. "Future plans" at the end outlines stories that were pushed out during discussions.

## Motivation

Organizations desire to reuse their existing enterprise log solutions to store container logs.  Providing a declarative mechanism by which administrators define a log destination simplifies their operational burden.  They are able to take advantage of log collection infrastructure with minimal configuration changes. The cluster logging stack is able to deploy a collector to each OKD node that is configured with the necessary permissions to collect the logs, add container metadata (e.g. labels) and ship them to the specified endpoint.

### Goals
The specific goals of this proposal are:

* Selectively forward `application`, `infrastructure` and `audit` inputs.
* Forward any combination of inputs to any combination of outputs.
* Send logs to endpoints not managed by the cluster logging infrastructure such as:
  * An Elasticsearch cluster (version 5 or 6)
  * An endpoint that accepts the fluent forward protocol (fluentd, fluentbit, others...).
  * An endpoint that accepts the syslog protocol via UDP, TCP or TLS.
  * A kafka broker
  * Others based on demand...
* Support TLS connections to outputs if so configured
* Provide a common, simplified, generic configuration for all output types.
  - connection URL, TLS, reconnect are always configured the same way.
  - limited access to essential output-specific features, e.g. setting a syslog facility.
* Configure inputs for the managed store (Elasticsearch) in the same way as external stores.
* Deploy a singleton cluster-scoped forwarder to manage forwarding for the cluster.
* Re-connect automatically if an output connection fails.

We will be successful when:

* an administrator is able to deploy their own log aggregation service
 - specifies this service as an output in the `ClusterLogForwarder` spec.
 - specifies the inputs (categories) to forward
 - the service receives the expected logs

### Non-Goals

* No secure storage for audit logs, only secure (TLS) delivery to a target system.
  - The user must ensure that the target system is secure and compliant with regulations.
  - The OpenShift Elasticsearch store is not guaranteed to comply with any such regulations.
* No direct access to the configuration schemes of target systems or the local collector
  - limited access to essential output-specific features, e.g. setting a syslog facility.
* Not intended to provide a complex routing solution as one might achieve by using a custom collector configuration or a messaging solution (e.g. kafka) - but is intended to allow forwarding to an such a system that is deployed externally.

## Proposal

Provide a declarative `pipeline` that associates a set of named `inputs` with a set of named `outputs`.

The following reserved input names are defined for the initial release:
* `application` - Container logs generated by user applications running on the platform, excluding `infrastructure` containers.
* `infrastructure` - Logs generated by infrastructure components running on the platform and OKD nodes (e.g. journal logs).  "infrastructure" applications are defined as any pods which run in namespaces: `openshift*`, `kube*`, `default`.
* `audit` - Logs generated by the nodes' auditd (/var/log/audit/audit.log), audit logs from the kubeapi-server and the openshift-apiserver. This will not be forwarded by default.

The following reserved output names are defined:
* `default` - the current Elasticsearch based store.

Users can define their own named outputs pointing to their target endpoints. An endpoint can be deployed on or off cluster.  Endpoints off-cluster may require adminstrators to perform additional actions in order for logs to be forwarded (e.g. secret creation, opening port, enable global proxy configuration)

The following output types are planned for initial support:

* Elasticsearch (v6.x) with/without TLS.
* Fluent `forward` with/without TLS.
* Syslog UDP, TCP, TLS.
* Kafka

### User Stories

#### As an OKD admin, I want to deploy only a collector, and have it ship logs off-cluster to my own aggregation solution

This is a typical example of organizations that desires to re-use their existing enterprise log solution.  We will succeeded if we are able to write logs to their logging service.

#### As an OKD admin, I want to aggregate application logs on-cluster and infra logs off-cluster

This is an example of an OKD cluster hosting solution where several organizations are each provided with a dedicated cluster.  The organization requires access to application container logs but the host requires access to the infra structure logs.

#### As an OKD admin, I need to forward my audit logs to a secure SIEM that meets government regulations

This is often required for industries such as the US public sector, healthcare or financials. The logs will be forwarded to a government approved SIEM through secure means (mutual TLS).

### Implementation Details

* A pipeline associates multiple input names with multiple output names.
* Users can define new outputs, output configuration includes:
  - `type` (e.g. `Syslog`,`Fluent`)
  - `url` used to connect to the endpoint.
  - `secret` referring to a secret object used for secure connections.
    - For a standard logging installation, secrets are in the `openshift-logging` namespace
    - You can omit the `namespace` field in the secretRef, the `openshift-logging` namespace is assumed. If you do include the `namespace` field it must be `openshift-logging`
    - Secrets not created by the cluster-logging-operator shall be created and managed by the administrator of the endpoint
    - NOTE: We use SecretReference rather than a simple name string for future flexibility.
* If no `ClusterLogForwarder` object exists, the default Elasticsearch instance is enabled (status quo)
* If a `ClusterLogForwarder` exists  the default Elasticsearch instance is disabled unless there is a `pipeline` with the `Default` output.

#### Security

* Server-authenticated TLS is enabled if `url` is a secure URL (e.g. 'https:')
* Client-authenticated TLS is enabled if `url` is secure *and* `secretRef` has keys `tls.crt`, `tls.key`, `ca-bundle.crt`
  - it is an error if `secretRef` is present but `url` is *not* secure, or the required 'tls.' keys are missing or invalid.
* An intentionally *insecure* output (no TLS) must have `insecure: true`
  - This is to avoid accidental insecure mis-configuration of an output that was intended to be secure.
* The user is responsible for creating and maintaining the secret objects
* The cluster logging operator is responsible for watching secrets and applying changes
  - e.g. if the user replaces a certificate or changes a password in a secret, the operator must re-connect affected outputs with the new credentials.

#### Reliability
* Output connections will automatically re-connect on disconnect.
* Changes to secrets will trigger automatic re-connect with new credentials.

#### Scale
* The cluster logging operator generates configuration for the collector
  - with a singleton ClusterLogForwarder this is unlikely to be a scaling problem.
* The actual connection and forwarding to remote endpoints is done by the collector
  - we rely on the collector to scale and perform.

#### Metrics
Note: the actual forwarding is done by the collector, so we can only provide metrics that are available from the collector.

Desirable metrics include:

* Counter:
  - Volume (bytes) per input, per pipeline, per output and total.
* Histogram/Summary:
  - Throughput (bytes/sec) per input, per pipeline, per output and total.
  - Read size: per input
  - Write size: per output
  - Latency (sec)
    - per pipeline: from read to written on all outputs.
    - per output: from read to written on this output.

#### Cluster Logging Operator
The `cluster-logging-operator` will use the `ClusterLogForwarder` configuration to:

* generate output configuration for the collector that respects all the pipelines.
* mount secrets in the collector daemonset as needed for each endpoint.
  - the controller ensures that collector configuration refers to the correct mounted secrets.
  - the exact location of secrets in the file-system is a controller implementation detail.

#### Collector
* The collectors will be modified to be remove endpoint config specific logic from the start script ; configuration is assumed to be correct and used as provided by the `cluster-logging-operator`
* Extract all configuration into the collector configuration.
* Extract the `run.sh` script from the collector image and mount into the deployed pod

### Risks and Mitigations
- The API and GA feature set are a close match to the Tech Preview API, which reduces the risk.
- We have starting-point implementations for fluent and syslog outputs.
- We have done experimental work on kafka outputs.

### Examples CRs for some use cases

#### As a cluster administrator, I want to forward to a remote service and also store logs locally.

I want a remote copy of logs, but also I want to continue using the default elasticsearch log store:
- I don't lose logs while the remote service is down.
- My local users can continue to view and query the logs locally.

```
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogForwarder"
spec:
  outputs:
   - name: SecureRemote
     type: syslog
     url: tls://secureforward.offcluster.com:9200
     secret:
        name: my_secrets # Must contain keys tls.key, tls.cert and ca.cert

  pipelines:
   - inputs: [ infrastructure, application, audit ]
     outputs: [ SecureRemote, Default ]
```

#### As a cluster administrator, I want to use a local syslog instance only, with no elasticsearch.

```
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogForwarder"
spec:
  outputs:
   - name: MyLogs
     type: syslog
     syslog:
       Facility: Local0
     url: localstore.example.com:9200
  pipelines:
   - inputs: [infrastructure, application, audit]
     outputs: [MyLogs]
```

#### As a cluster administrator, I want to clearly separate where the logging stack forwards infrastructure and/or audit related logs.

```
apiVersion: "logging.openshift.io/v1"
kind: "ClusterLogForwarder"
spec:
  outputs:
   - name: MyInfra ...
   - name: MyApp ...
   - name: MyAudit ...
  pipelines:
   - inputs: [infrastructure]
     outputs: [MyInfra]
   - inputs: [application]
     outputs: [MyApp]
   - inputs: [audit]
     outputs: [MyAudit]
```
### As a Red Hat SRE who operates OSD clusters, I want a mechanism to protect my configuration (e.g. audit log forwarding, infra logs) from non SRE administrators  of OSD but at the same time give them the opportunity to configure their own log forwarding for applications. ###

This use case will be resolved by an admissions webhook, outside of the forwarder. Such a webhook will
* refuse requests to create/modify pipelines with `infrastructure` or `audit` inputs except for a special role/user representing the SRC
* allow requests to create/modify pipelines with only `application` inputs as usual.

### Test Plan

#### Regression testing
Translate all existing TP tests to new API, translation should be simple, tests should pass.

#### Unit testing
* Log forwarding will add unit tests to provide adequate coverage for all changes
* BDD unit testing will be added to unit testing to make tests goals more expliit,  readable, and obvious
* Use `go test -cover` and related tools to measure coverage https://blog.golang.org/cover

#### Functional testing
Go tests that run (in sub-processes or goroutines):
- a collector instance
- a dummy log receiver
- a simulated container generating logs

Verify configurations:
* Pipelines with multiple, overlapping outputs.
* TLS server and client authentication.
* Tests for all output types
* Verify reconnect
* Error scenarios.

Note: by driving functional tests from `go test` we can get coverage stats integrated with the unit tests.

#### Integration and E2E tests
* Tests to verify no change in behavior with
  - No ClusterLogForwarder object deployed.
  - A ClusterLogForwarder object with this configuration:
    ```pipelines: {inputs: [infrastructure, application, audit], outputs: [Default]}```
* Tests to verify log forwarding is writing logs to an Elasticsearch instance not managed by cluster logging
* Tests to verify log forwarding is writing logs to a fluentd instance that is not managed by cluster logging
* Tests to verify log forwarding is writing logs to a syslog instance that is not managed by cluster logging

#### Scale and stress testing
* Run selected E2E tests under stress conditions:
  - many nodes
  - many containers
  - high-volume log streams
  - many outputs
* Find breaking points.
* Fix bugs that show up under stress.
* Optimize performance bottlenecks.

### Graduation Criteria

##### Tech Preview -> GA

Essential:
- Refactor existing TP implementation to implement new API.
- Implement new GA output types.
- Sufficient test coverage (upgrade, tech. preview migration, downgrade, scale)
- Available by default without tech preview annotation
- End user documentation.

### Upgrade / Downgrade Strategy

#### Upgrade

After upgrading the cluster-logging-operator the Tech Preview API will become inactive (abandon in place)

For upgrade from a Tech preview we are not obliged to do more than that, but if time permits the operator will:
* detect an existing tech-preview instance
* generate an equivalent GA configuration (also considering the TP enable/disable annotations to include/exclude a Default pipeline)
* deploy the equivalent GA API instance
* mark the old instance as inactive with an informative status.

#### Downgrade
Downgrades should be discouraged unless we know for certain the Elasticsearch version managed by cluster logging is the same version.  There is risk that Elasticsearch may have migrated data that is unreadable by an older version.

### Version Skew Strategy

Version skew is not relevant to the GA proposal because the operands will not change, only the way the operator configures them. Logging is deployed as an OLM managed operator and component versions are set in a versioned operator deployment.

In future upgrades where operator+operand versions may be temporarily mismatched, we will need to handle the version skew issues.


## Implementation History

| release|Description|
|---|---|
|4.3| **Tech Preview** - Initial release supporting `Elasticsearch` and Fluentd `forward` 

## Drawbacks
Drawbacks to providing this enhancement are:
* Increased exposure to issues being raised by customers for things outside the control of the cluster logging team
  * What happens when the customer managed endpoint is down?  How well does the collector handle the back pressure? When do logs get dropped because they can not be shipped?
* Setting customer expectations of the capabilities of log forwarding and guarantees (e.g. rates, delivery, reliability, message loss)

## Alternatives

Provide a recipe for customer's to deploy their own log collector to move the responsibility to the customer.

## Infrastructure Needed
* Future target endpoints may require special infrastructure or licensing to properly test.
* Scale and stress tests require intensive use of a large cluster for an extended period of time.

## Future plans
As well as serving the current GA requirements, the log forwarding API has been designed with the following future requirements in mind.

### Stand-alone log forwarding. ###

Deploy log forwarding without deploying the entirety of the cluster logging infrastructure (e.g. Kibana, Elasticsearch) Forwarding will be a stand-alone system independent of any log store. This decoupling will let us test forwarding separately, and let customers to switch off our managed store entirely while still using a managed and supported forwarder.

### As a team lead (tenant), I’d like to configure secure log forwarding to the tool of my team's choice, separate from global config. ###

Introduce a namespace-scoped LogForwarder. The API is a restricted version of the ClusterLogForwarder API:
- can't use infrastructure or audit inputs
- can only forward logs from own namespace.

Although there could be many `LogForwarder` objects, there is still only one collector. The operator would join all the configurations and compile them to a single collector configuration. It would also enforce the limitations of namespace-scoped forwarders.

### I want to configure log forwarding to include/exclude logs on k8s labels. ###

Allow user-defined named inputs in addition to the built in application, infrastructure, audit.
User inputs can select logs based on:
* K8s label selector maps and/or expressions.
* Namespaces

User defined inputs could also be extended to allow per-record filtering and transformations (e.g. using regular expressions), but we haven't though much about that yet.

### I want many namespace-scoped forwarders to share the same remote logging connection ###
Having every namespace define it's own log forwarding outputs may create a large number of connections from the underlying collector. In many cases you would like to define a single Output destination (e.g. for "ImportantApplications"), but allow each namespace to define for itself which applications are "Important" by creating pipelines to a shared ImportantApplications output.

The solution is to define a "shared output" API. This has the same configuration as an `output` entry in the ClusterLogForwarder API, but can be deployed as a separate object. Any forwarder configuration can refer to the output as "<namespace>/<name>", the cluster logging controller will collect all pipelines referring to that name, and generate collector configuration
to do all the requested forwarding over a single connection.

Security consideration: we need to restrict use of an Output either by role or namespace, needs investigation.
